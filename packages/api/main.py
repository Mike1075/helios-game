from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import json
import os
import requests
from typing import List, Dict, Any, AsyncGenerator
import asyncio
from pydantic import BaseModel

app = FastAPI(title="Helios Agent Core", version="0.1.0")

# CORS middleware for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Vercelç¯å¢ƒå˜é‡ (ç”±Mikeåœ¨Verceläº‘ç«¯é…ç½®)
VERCEL_AI_GATEWAY_URL = os.environ.get("VERCEL_AI_GATEWAY_URL")
VERCEL_AI_GATEWAY_API_KEY = os.environ.get("VERCEL_AI_GATEWAY_API_KEY")

class Message(BaseModel):
    role: str
    content: str
    id: str = None

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4o-mini"
    stream: bool = True

@app.get("/")
async def root():
    return {"message": "Helios Agent Core is running", "version": "0.1.0"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "service": "helios-agent-core"}

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """
    AIèŠå¤©ç«¯ç‚¹ï¼Œç›´æ¥è°ƒç”¨Vercel AI Gatewayï¼Œæ”¯æŒæµå¼è¾“å‡º
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not VERCEL_AI_GATEWAY_URL or not VERCEL_AI_GATEWAY_API_KEY:
            # æœ¬åœ°å¼€å‘ç¯å¢ƒ - è¿”å›æ¨¡æ‹Ÿå“åº”
            return StreamingResponse(
                mock_streaming_response(request.messages, request.model),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                }
            )
        
        # ç”Ÿäº§ç¯å¢ƒ - è°ƒç”¨Vercel AI Gateway
        headers = {
            "Authorization": f"Bearer {VERCEL_AI_GATEWAY_API_KEY}",
            "Content-Type": "application/json"
        }

        # å°†æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºOpenAI APIæ ¼å¼
        openai_messages = [
            {"role": msg.role, "content": msg.content} 
            for msg in request.messages
        ]

        payload = {
            "model": request.model,
            "messages": openai_messages,
            "stream": True,  # å¼ºåˆ¶ä½¿ç”¨æµå¼è¾“å‡º
            "max_tokens": 2048,
            "temperature": 0.7
        }

        # è°ƒç”¨Vercel AI Gateway
        return StreamingResponse(
            stream_ai_gateway_response(headers, payload),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache", 
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

async def mock_streaming_response(messages: List[Message], model: str = "gpt-4o-mini") -> AsyncGenerator[str, None]:
    """
    æœ¬åœ°å¼€å‘ç¯å¢ƒçš„æ¨¡æ‹Ÿæµå¼å“åº” - ç¬¦åˆAI SDKæ ¼å¼
    """
    user_message = messages[-1].content if messages else "æ— æ¶ˆæ¯"
    
    mock_response = f"""ä½ å¥½ï¼æˆ‘æ˜¯Helios AIåŠ©æ‰‹ã€‚

ğŸŒŸ å½“å‰çŠ¶æ€ï¼šæœ¬åœ°å¼€å‘æ¨¡å¼ï¼ˆæ¨¡æ‹Ÿå“åº”ï¼‰
ğŸ¤– é€‰æ‹©æ¨¡å‹ï¼š{model}
ğŸ“ æ¶ˆæ¯æ•°é‡ï¼š{len(messages)}
ğŸ’¬ æœ€åæ¶ˆæ¯ï¼š"{user_message}"

è¿™æ˜¯æµå¼è¾“å‡ºæ¼”ç¤ºã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘å°†é€šè¿‡Vercel AI Gatewayè¿æ¥åˆ°çœŸå®çš„{model}æ¨¡å‹ã€‚

âœ¨ Heliosé¡¹ç›®æ ¸å¿ƒç‰¹æ€§ï¼š
- ğŸ”® ä¿¡å¿µç³»ç»Ÿé©±åŠ¨çš„å¯¹è¯ä½“éªŒ
- ğŸ¤– é«˜æ€§èƒ½NPCä»£ç†æ ¸å¿ƒ
- ğŸª å›å“ä¹‹å®¤è‡ªæˆ‘åæ€æœºåˆ¶  
- ğŸ­ æ™ºèƒ½å¯¼æ¼”å¼•æ“

ğŸ“ é›¶ä¿¡ä»»å¼€å‘ï¼šæœ¬åœ°ç¼–ç  + äº‘ç«¯æµ‹è¯•
ğŸš€ ç°åœ¨ä½ çœ‹åˆ°çš„æ˜¯æ¨¡æ‹Ÿçš„é€å­—ç¬¦æµå¼è¾“å‡ºæ•ˆæœ..."""

    # æ¨¡æ‹ŸAI SDKçš„æµå¼è¾“å‡ºæ ¼å¼
    for i, char in enumerate(mock_response):
        chunk = {
            "choices": [{
                "delta": {"content": char},
                "index": 0,
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(chunk)}\n\n"
        await asyncio.sleep(0.03)  # æ¨¡æ‹ŸçœŸå®AIçš„å“åº”é€Ÿåº¦
    
    # å‘é€ç»“æŸæ ‡å¿—
    final_chunk = {
        "choices": [{
            "delta": {},
            "index": 0,
            "finish_reason": "stop"
        }]
    }
    yield f"data: {json.dumps(final_chunk)}\n\n"
    yield "data: [DONE]\n\n"

async def stream_ai_gateway_response(headers: dict, payload: dict) -> AsyncGenerator[str, None]:
    """
    ä»Vercel AI Gatewayè·å–æµå¼å“åº”
    """
    try:
        # è°ƒç”¨Vercel AI Gateway
        response = requests.post(
            f"{VERCEL_AI_GATEWAY_URL}/v1/chat/completions",
            headers=headers,
            json=payload,
            stream=True
        )
        response.raise_for_status()

        # å¤„ç†æµå¼å“åº”
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                    if data.strip() == '[DONE]':
                        yield "data: [DONE]\n\n"
                        break
                    
                    try:
                        # ç›´æ¥è½¬å‘AI Gatewayçš„å“åº”
                        json.loads(data)  # éªŒè¯JSONæ ¼å¼
                        yield f"data: {data}\n\n"
                    except json.JSONDecodeError:
                        continue

    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_chunk = {
            "choices": [{
                "delta": {"content": f"\n\nâŒ è¿æ¥AIæœåŠ¡å¤±è´¥: {str(e)}"},
                "index": 0,
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

@app.get("/api/models")
async def get_available_models():
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    # è¯»å–æœ¬åœ°æ¨¡å‹é…ç½®æ–‡ä»¶
    try:
        import json
        models_file = "../../docs/available-models.json"
        with open(models_file, 'r', encoding='utf-8') as f:
            models = json.load(f)
        return models
    except FileNotFoundError:
        return {
            "openai": {
                "gpt-4o-mini": {
                    "provider": "openai",
                    "model": "gpt-4o-mini",
                    "description": "Fast and cost-effective OpenAI model",
                    "streaming": True
                }
            }
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)