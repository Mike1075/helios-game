from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import json
import os
import requests
from typing import List, Dict, Any, AsyncGenerator
import asyncio
from pydantic import BaseModel

app = FastAPI(title="Helios Agent Core", version="0.1.0")

# CORS middleware for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç¯å¢ƒå˜é‡
VERCEL_AI_GATEWAY_URL = os.environ.get("VERCEL_AI_GATEWAY_URL")
VERCEL_AI_GATEWAY_API_KEY = os.environ.get("VERCEL_AI_GATEWAY_API_KEY")

class Message(BaseModel):
    role: str
    content: str
    id: str = None

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4o-mini"
    stream: bool = True

@app.get("/")
async def root():
    return {"message": "Helios Agent Core is running", "version": "0.1.0"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "service": "helios-agent-core"}

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """
    AIèŠå¤©ç«¯ç‚¹ï¼Œæ”¯æŒæµå¼è¾“å‡º
    """
    try:
        # å¦‚æœåœ¨æœ¬åœ°å¼€å‘ç¯å¢ƒï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
        if not VERCEL_AI_GATEWAY_URL or not VERCEL_AI_GATEWAY_API_KEY:
            return StreamingResponse(
                mock_streaming_response(request.messages),
                media_type="text/plain"
            )
        
        # æ„é€ è¯·æ±‚åˆ°Vercel AI Gateway
        headers = {
            "Authorization": f"Bearer {VERCEL_AI_GATEWAY_API_KEY}",
            "Content-Type": "application/json"
        }

        # å°†æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºOpenAI APIæ ¼å¼
        openai_messages = [
            {"role": msg.role, "content": msg.content} 
            for msg in request.messages
        ]

        payload = {
            "model": request.model,
            "messages": openai_messages,
            "stream": request.stream,
            "max_tokens": 2048,
            "temperature": 0.7
        }

        # å‘é€æµå¼è¯·æ±‚åˆ°AI Gateway
        if request.stream:
            return StreamingResponse(
                stream_ai_response(headers, payload),
                media_type="text/plain"
            )
        else:
            # éæµå¼å“åº”
            response = requests.post(
                f"{VERCEL_AI_GATEWAY_URL}/v1/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            
            result = response.json()
            return {
                "content": result["choices"][0]["message"]["content"],
                "role": "assistant"
            }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

async def mock_streaming_response(messages: List[Message]) -> AsyncGenerator[str, None]:
    """
    æœ¬åœ°å¼€å‘ç¯å¢ƒçš„æ¨¡æ‹Ÿæµå¼å“åº”
    """
    mock_response = f"""ä½ å¥½ï¼æˆ‘æ˜¯Helios AIåŠ©æ‰‹ã€‚

å½“å‰çŠ¶æ€ï¼šæœ¬åœ°å¼€å‘æ¨¡å¼ï¼ˆæ¨¡æ‹Ÿå“åº”ï¼‰

ä½ åˆšæ‰å‘é€çš„æ¶ˆæ¯æ•°é‡ï¼š{len(messages)}
æœ€åä¸€æ¡æ¶ˆæ¯ï¼š"{messages[-1].content if messages else 'æ— '}"

è¿™æ˜¯ä¸€ä¸ªæµå¼è¾“å‡ºçš„æ¼”ç¤ºã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘å°†è¿æ¥åˆ°çœŸå®çš„AIæ¨¡å‹ã€‚

âœ¨ Heliosé¡¹ç›®ç‰¹æ€§ï¼š
- ğŸ”® ä¿¡å¿µç³»ç»Ÿé©±åŠ¨çš„å¯¹è¯
- ğŸ¤– NPCä»£ç†æ ¸å¿ƒ
- ğŸª å›å“ä¹‹å®¤è‡ªçœ
- ğŸ­ å¯¼æ¼”å¼•æ“

ç°åœ¨ä½ çœ‹åˆ°çš„æ˜¯é€å­—ç¬¦çš„æµå¼è¾“å‡ºæ•ˆæœ..."""

    # æ¨¡æ‹Ÿé€å­—ç¬¦æµå¼è¾“å‡º
    for char in mock_response:
        yield f"data: {json.dumps({'content': char})}\n\n"
        await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
    
    yield "data: [DONE]\n\n"

async def stream_ai_response(headers: dict, payload: dict) -> AsyncGenerator[str, None]:
    """
    ä»AI Gatewayè·å–æµå¼å“åº”
    """
    try:
        response = requests.post(
            f"{VERCEL_AI_GATEWAY_URL}/v1/chat/completions",
            headers=headers,
            json=payload,
            stream=True
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                    if data == '[DONE]':
                        yield "data: [DONE]\n\n"
                        break
                    
                    try:
                        json_data = json.loads(data)
                        if 'choices' in json_data and len(json_data['choices']) > 0:
                            delta = json_data['choices'][0].get('delta', {})
                            if 'content' in delta:
                                yield f"data: {json.dumps({'content': delta['content']})}\n\n"
                    except json.JSONDecodeError:
                        continue

    except Exception as e:
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.get("/api/models")
async def get_available_models():
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    # è¯»å–æœ¬åœ°æ¨¡å‹é…ç½®æ–‡ä»¶
    try:
        import json
        models_file = "../../docs/available-models.json"
        with open(models_file, 'r', encoding='utf-8') as f:
            models = json.load(f)
        return models
    except FileNotFoundError:
        return {
            "openai": {
                "gpt-4o-mini": {
                    "provider": "openai",
                    "model": "gpt-4o-mini",
                    "description": "Fast and cost-effective OpenAI model",
                    "streaming": True
                }
            }
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)